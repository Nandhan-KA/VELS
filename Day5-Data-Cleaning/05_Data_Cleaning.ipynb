{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8fddd0",
   "metadata": {},
   "source": [
    "# Day 5: Data Cleaning\n",
    "\n",
    "This notebook covers outlier detection and data scaling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b511e8b",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da633d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e3282",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Dataset with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal data with outliers\n",
    "normal_data = np.random.normal(100, 15, 1000)\n",
    "outliers = np.random.uniform(200, 300, 50)\n",
    "data = np.concatenate([normal_data, outliers])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'values': data\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(df['values'])\n",
    "plt.title('Distribution of Values with Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a308f",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# IQR Method\n",
    "outliers_iqr, lb, ub = detect_outliers_iqr(df['values'])\n",
    "print(\"IQR Method:\")\n",
    "print(f\"Number of outliers: {len(outliers_iqr)}\")\n",
    "print(f\"Lower bound: {lb:.2f}\")\n",
    "print(f\"Upper bound: {ub:.2f}\")\n",
    "\n",
    "# Z-score Method\n",
    "outliers_zscore = detect_outliers_zscore(df['values'])\n",
    "print(\"\\nZ-score Method:\")\n",
    "print(f\"Number of outliers: {len(outliers_zscore)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd781f",
   "metadata": {},
   "source": [
    "## 4. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac064be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, method='clip'):\n",
    "    if method == 'clip':\n",
    "        # Winsorization\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return data.clip(lower=lower_bound, upper=upper_bound)\n",
    "    elif method == 'remove':\n",
    "        # Remove outliers\n",
    "        z_scores = np.abs(stats.zscore(data))\n",
    "        return data[z_scores <= 3]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'clip' or 'remove'\")\n",
    "\n",
    "# Apply both methods\n",
    "df['values_clipped'] = handle_outliers(df['values'], method='clip')\n",
    "df['values_removed'] = handle_outliers(df['values'], method='remove')\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].boxplot(df['values'])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].boxplot(df['values_clipped'])\n",
    "axes[1].set_title('After Winsorization')\n",
    "axes[2].boxplot(df['values_removed'])\n",
    "axes[2].set_title('After Removing Outliers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e69437",
   "metadata": {},
   "source": [
    "## 5. Data Scaling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ae197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with multiple features\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'feature1': np.random.normal(100, 15, n_samples),\n",
    "    'feature2': np.random.exponential(50, n_samples),\n",
    "    'feature3': np.random.uniform(0, 1000, n_samples)\n",
    "}\n",
    "df_scaling = pd.DataFrame(data)\n",
    "\n",
    "# Apply different scaling methods\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_dfs = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_dfs[name] = pd.DataFrame(\n",
    "        scaler.fit_transform(df_scaling),\n",
    "        columns=df_scaling.columns\n",
    "    )\n",
    "\n",
    "# Compare distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Features with Different Scaling Methods')\n",
    "\n",
    "# Original data\n",
    "sns.boxplot(data=df_scaling, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Original Data')\n",
    "\n",
    "# Scaled data\n",
    "positions = [(0, 1), (1, 0), (1, 1)]\n",
    "for (name, scaled_df), pos in zip(scaled_dfs.items(), positions):\n",
    "    sns.boxplot(data=scaled_df, ax=axes[pos[0], pos[1]])\n",
    "    axes[pos[0], pos[1]].set_title(f'{name.capitalize()} Scaling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for name, scaled_df in scaled_dfs.items():\n",
    "    print(f\"\\n{name.capitalize()} Scaling:\")\n",
    "    print(scaled_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f102a",
   "metadata": {},
   "source": [
    "## 6. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c552ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skewed data\n",
    "skewed_data = np.random.lognormal(0, 1, 1000)\n",
    "\n",
    "# Apply log transformation\n",
    "log_transformed = np.log1p(skewed_data)  # log1p is log(1+x)\n",
    "\n",
    "# Plot original vs transformed data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original data\n",
    "sns.histplot(skewed_data, ax=axes[0])\n",
    "axes[0].set_title('Original Skewed Data')\n",
    "\n",
    "# Log-transformed data\n",
    "sns.histplot(log_transformed, ax=axes[1])\n",
    "axes[1].set_title('Log-transformed Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nOriginal Data:\")\n",
    "print(pd.Series(skewed_data).describe().round(2))\n",
    "print(\"\\nLog-transformed Data:\")\n",
    "print(pd.Series(log_transformed).describe().round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
